# eiCLIP_pipeline v6
# author Melina Klostermann

from pickle import TRUE
import pickle
import string
import sys
import yaml
import os
import csv
import json
import re

#print(pickle.__)
print(string.__name__)
print(sys.__package__)
print(yaml.__version__)
#print(os._)
print(csv.__version__)
print(json.__package__)
print(re.__package__)

# importing  all the
# functions defined in test.py
# sys.path.append("workflow/rules/")
# from definitions import *

#################################################
#################################################
# parameter from config file
#################################################
#################################################


ARGS = sys.argv
#print(ARGS)
#CON_P = ARGS[ARGS.index("--configfile") + 1]
# pattern = r"\.yaml\s*>$"
# replacement = "_full.yaml"
# CONFIG_PATH = re.sub(pattern, replacement, CON_P)
# Use workflow.basedir to get the actual Snakefile directory instead of config["snakebase"]
# which may have incorrect paths like /workspace/racoon_clip
SNAKE_PATH = os.path.dirname(workflow.basedir)
print(SNAKE_PATH)


# configfile: DEF_CONFIG
# with open("DEF_CONFIG", "r") as f:
#     config_default = yaml.safe_load(f)

WDIR=config["wdir"]
# Handle glob patterns in infiles (e.g., *.fastq, *.fastq.gz)
import glob
import os
infiles_raw = config["infiles"]
if "*" in infiles_raw:
    # Expand glob pattern
    SAMPLES_FULL = glob.glob(infiles_raw)
    if not SAMPLES_FULL:
        raise ValueError(f"ERROR: No files found matching pattern: {infiles_raw}")
    # Sort for consistent ordering
    SAMPLES_FULL.sort()
else:
    # Split space-separated file list
    SAMPLES_FULL = infiles_raw.split()
    # Check that all files exist and are readable
    missing_files = []
    unreadable_files = []
    for file_path in SAMPLES_FULL:
        if not os.path.exists(file_path):
            missing_files.append(file_path)
        elif not os.access(file_path, os.R_OK):
            unreadable_files.append(file_path)
    
    if missing_files:
        raise ValueError(f"ERROR: The following input files do not exist: {', '.join(missing_files)}")
    
    if unreadable_files:
        raise ValueError(f"ERROR: No permission to read the following files: {', '.join(unreadable_files)}")

print("samples")
print(SAMPLES_FULL)

# Auto-generate sample names from file paths, or use config if specified
if "samples" in config and config["samples"]:
    # Use user-specified sample names
    SAMPLES = config["samples"].split()
    
    # Auto-generate sample names to compare against user-specified names
    auto_generated_samples = []
    for file_path in SAMPLES_FULL:
        # Extract filename without directory and extensions
        basename = os.path.basename(file_path)
        # Remove common file extensions
        sample_name = basename
        for ext in ['.fastq.gz', '.fq.gz', '.fastq', '.fq']:
            if sample_name.endswith(ext):
                sample_name = sample_name[:-len(ext)]
                break
        auto_generated_samples.append(sample_name)
    
else:
    # Auto-generate sample names from file paths
    SAMPLES = []
    for file_path in SAMPLES_FULL:
        # Extract filename without directory and extensions
        basename = os.path.basename(file_path)
        # Remove common file extensions
        sample_name = basename
        for ext in ['.fastq.gz', '.fq.gz', '.fastq', '.fq']:
            if sample_name.endswith(ext):
                sample_name = sample_name[:-len(ext)]
                break
        SAMPLES.append(sample_name)
    
    print("INFO: Auto-generated sample names from input files:")
    for file_path, sample_name in zip(SAMPLES_FULL, SAMPLES):
        print(f"  {file_path} -> {sample_name}")

print("Final samples:")
print(SAMPLES)

# Create mapping from sample names to full file paths
SAMPLE_TO_FILE = dict(zip(SAMPLES, SAMPLES_FULL))
print("Sample to file mapping:")
for sample, file_path in SAMPLE_TO_FILE.items():
    print(f"  {sample} -> {file_path}")

# Keep INPATH for backward compatibility (assumes files in same directory)
INPATH=[os.path.dirname(path) for path in SAMPLES_FULL][0]
DEMUX=config["demultiplex"] == "True" or config["demultiplex"] == "true" or config["demultiplex"] == True

# Validate sample names against file names if samples were user-specified and demux is False
if "samples" in config and config["samples"] and DEMUX == False:
    # Re-create auto_generated_samples for validation
    auto_generated_samples = []
    for file_path in SAMPLES_FULL:
        basename = os.path.basename(file_path)
        sample_name = basename
        for ext in ['.fastq.gz', '.fq.gz', '.fastq', '.fq']:
            if sample_name.endswith(ext):
                sample_name = sample_name[:-len(ext)]
                break
        auto_generated_samples.append(sample_name)
    
    # Verify that user-specified names match auto-generated names
    if set(SAMPLES) != set(auto_generated_samples):
        raise ValueError( User-specified sample names do not match auto-generated names from file paths.")
        print("Auto-generated names:", auto_generated_samples)
        print("User-specified names:", SAMPLES)
        raise ValueError("Sample names must match the base names of the input files (without path and extensions)")

QUAL_BC=(config["quality_filter_barcodes"] == "True" or config["quality_filter_barcodes"] == "true" or config["quality_filter_barcodes"] == True) and config["experiment_type"]!= "noBarcode_noUMI"
GZ= os.path.splitext(SAMPLES_FULL[0])[1] == ".gz" 
#ENCODE=config["encode"] == "True"
ADAPTER_FILE=config["adapter_file"] #if "adapter_file" in config else config_default["adapter_file"]
TRIM=config["adapter_trimming"] == "True" or config["adapter_trimming"] == "true" or config["adapter_trimming"] == True
FASTQ_SCREEN=(config["fastqScreen"] == "True" or config["fastqScreen"] == "true" or config["fastqScreen"] == True)

# Validate FastQ Screen config if enabled
if FASTQ_SCREEN:
    if not config["fastqScreen_config"]:
        raise ValueError(f"ERROR: FastQ Screen is enabled but no configuration file specified. Please set fastqScreen_config in your config file.")
    elif not os.path.exists(config["fastqScreen_config"]):
        raise ValueError(f"ERROR: FastQ Screen configuration file not found: {config['fastqScreen_config']}")
TRIM3=config["trim3"] == "True" or config["trim3"] == "true" or config["trim3"] == True or config["experiment_type"] == "iCLIP3"
DEDUP=(config["deduplicate"] == "True" or config["deduplicate"] == "true" or config["deduplicate"] == True) and config["experiment_type"]!= "noBarcode_noUMI"
MIR_starts=config["mir_starts_allowed"].split()
STAR_INDEX=config["star_index"] if config["star_index"] != "" else None

# Validate reference files

# Check GTF file
gtf_file = config["gtf"]
if not os.path.exists(gtf_file):
    raise ValueError(f"ERROR: GTF file does not exist: {gtf_file}")
if not os.access(gtf_file, os.R_OK):
    raise ValueError(f"ERROR: No permission to read GTF file: {gtf_file}")
if not gtf_file.lower().endswith('.gtf'):
    raise ValueError(f"ERROR: GTF file must end with .gtf extension: {gtf_file}")

# Check genome FASTA file
genome_fasta = config["genome_fasta"]
if not os.path.exists(genome_fasta):
    raise ValueError(f"ERROR: Genome FASTA file does not exist: {genome_fasta}")
if not os.access(genome_fasta, os.R_OK):
    raise ValueError(f"ERROR: No permission to read genome FASTA file: {genome_fasta}")
if not genome_fasta.lower().endswith(('.fasta', '.fa')):
    raise ValueError(f"ERROR: Genome FASTA file must end with .fasta or .fa extension: {genome_fasta}")

# Check STAR index directory if specified
if STAR_INDEX is not None:
    if not os.path.exists(STAR_INDEX):
        raise ValueError(f"ERROR: STAR index directory does not exist: {STAR_INDEX}")
    if not os.path.isdir(STAR_INDEX):
        raise ValueError(f"ERROR: STAR index path is not a directory: {STAR_INDEX}")
    if not os.access(STAR_INDEX, os.R_OK):
        raise ValueError(f"ERROR: No permission to read STAR index directory: {STAR_INDEX}")
    # Check if directory contains STAR index files
    star_files = ['Genome', 'SA', 'SAindex']
    missing_star_files = []
    for star_file in star_files:
        star_path = os.path.join(STAR_INDEX, star_file)
        if not os.path.exists(star_path):
            missing_star_files.append(star_file)
    if missing_star_files:
        raise ValueError(f"ERROR: STAR index directory is missing required files: {', '.join(missing_star_files)}. Directory: {STAR_INDEX}")




# get groups
GROUPS = set()
if config["experiment_group_file"] is not None and os.path.exists(config["experiment_group_file"]):
    with open(config["experiment_group_file"], "r") as file:
        for line in file:
            columns = line.strip().split()
            if columns:  # Check if the line is not empty
                first_column = columns[0]
                GROUPS.add(first_column)
else:
    GROUPS.add("all_samples")
    
print("groups:") 
print(GROUPS)

# get experiment info
# one of "iCLIP", "iCLIP2", "eCLIP", "eCLIP_ENCODE" or "other"

TYPE = config["experiment_type"]


def get_barcode_experiment_info():
    if TYPE== "iCLIP":
        info = {
            "barcodeLength": 4,
            "umi1_len": 3, # antisense of used barcodes --> this is the 3' umi of the original barcode
            "umi2_len": 2,
            "total_barcode_len": 9,
            "encode": False,
            "miR": False
        }
    elif TYPE== "iCLIP2":
        info = {
            "barcodeLength": 6, 
            "umi1_len": 5, # antisense of used barcodes --> this is the 3' umi of the original barcode
            "umi2_len": 4,
            "total_barcode_len": 15,
            "encode": False,
            "miR": False
        }
    elif TYPE== "eCLIP_5ntUMI":
        info = {
            "barcodeLength": 0, 
            "umi1_len": 5, # antisense of used barcodes --> this is the 3' umi of the original barcode
            "umi2_len": 0,
            "total_barcode_len": 5,
            "encode": False,
            "miR": False
        }
    elif TYPE== "eCLIP_10ntUMI":
        info = {
            "barcodeLength": 0, 
            "umi1_len": 10, # antisense of used barcodes --> this is the 3' umi of the original barcode
            "umi2_len": 0,
            "total_barcode_len": 10,
            "encode": False,
            "miR": False
        }
    elif TYPE== "eCLIP_ENCODE_5ntUMI":
        info = {
            "barcodeLength": 0, 
            "umi1_len": 5, # antisense of used barcodes --> this is the 3' umi of the original barcode
            "umi2_len": 0,
            "total_barcode_len": 5, # if already demux = umi1_len
            "encode": True,
            "miR": False    
        }
    elif TYPE== "eCLIP_ENCODE_10ntUMI":
        info = {
            "barcodeLength": 0, 
            "umi1_len": 10, # antisense of used barcodes --> this is the 3' umi of the original barcode
            "umi2_len": 0,
            "total_barcode_len": 10, # if already demux = umi1_len
            "encode": True,
            "miR": False    
        }
    elif TYPE== "noBarcode_noUMI":
        info = {
            "barcodeLength": 0, 
            "umi1_len": 0, # antisense of used barcodes --> this is the 3' umi of the original barcode
            "umi2_len": 0,
            "total_barcode_len": 0, # if already demux = umi1_len
            "encode": False,
            "miR": False
        }
    elif TYPE== "miReCLIP":
        info = {
            "barcodeLength": 10, 
            "umi1_len": 10, # antisense of used barcodes --> this is the 3' umi of the original barcode
            "umi2_len": 0,
            "total_barcode_len": 10, # if already demux = umi1_len
            "encode": False,
            "miR": True
        }
    elif TYPE== "iCLIP3":
        info = {
            "barcodeLength": 0, 
            "umi1_len": 9, # antisense of used barcodes --> this is the 3' umi of the original barcode
            "umi2_len": 0,
            "total_barcode_len": 10, # if already demux = umi1_len
            "encode": False,
            "miR": False,
            "trim3": True,
            "trim3_len": 3
        }
    elif TYPE== "other": 
        if config["barcodeLength"] == "":
            raise ValueError("Input Error: barcodeLength is empty. If experiment_type is set to other, you need to specify barcodeLength, umi1_len, umi2_len, total_barcode_len and total_barcode_len manually. Consider using one of the types iCLIP/iCLIP2/eCLIP/eCLIP_ENCODE, if you data is from iCLIP or eCLIP.")
        if config["umi1_len"] == "":
            raise ValueError("Input Error: umi1_len is empty. If experiment_type is set to other, you need to specify barcodeLength, umi1_len, umi2_len, total_barcode_len and total_barcode_len manually. Consider using one of the types iCLIP/iCLIP2/eCLIP/eCLIP_ENCODE, if you data is from iCLIP or eCLIP.")
        if config["umi2_len"] == "":
            raise ValueError("Input Error: umi2_len is empty. If experiment_type is set to other, you need to specify barcodeLength, umi1_len, umi2_len, total_barcode_len and total_barcode_len manually. Consider using one of the types iCLIP/iCLIP2/eCLIP/eCLIP_ENCODE, if you data is from iCLIP or eCLIP.")
        if config["total_barcode_len"] == "":
            raise ValueError("Input Error: total_barcode_len is empty. If experiment_type is set to other, you need to specify barcodeLength, umi1_len, umi2_len, total_barcode_len and total_barcode_len manually. Consider using one of the types iCLIP/iCLIP2/eCLIP/eCLIP_ENCODE, if you data is from iCLIP or eCLIP.")

        
        info = {
            "barcodeLength": config["barcodeLength"], # if already demux = umi1_len
            "umi1_len": config["umi1_len"], # antisense of used barcodes --> this is the 3' umi of the original barcode
            "umi2_len": config["umi2_len"],
            "total_barcode_len": config["total_barcode_len"],
            "encode": config["encode"],
            "miR": False
        }
    else:
        raise ValueError('Input Error: experiment_type must be one of "iCLIP", "iCLIP2", "eCLIP_5ntUMI", "eCLIP_10ntUMI", "eCLIP_ENCODE_5ntUMI", "eCLIP_ENCODE_10ntUMI", "noBarcode_noUMI" or "other".')   
    return info

print(get_barcode_experiment_info()["encode"])
ENCODE = get_barcode_experiment_info()["encode"]
MIR = get_barcode_experiment_info()["miR"]

# Check for ENCODE and quality filtering compatibility
if ENCODE == True and QUAL_BC == True:
    print("WARNING: Quality filtering is not compatible with ENCODE data types.")
    print("Setting quality_filter_barcodes to False for ENCODE experiment type.")
    QUAL_BC = False

# Check for ENCODE and demultiplexing compatibility
if ENCODE == True and DEMUX == True:
    raise ValueError("ERROR: ENCODE data cannot be demultiplexed. ENCODE experiment types (eCLIP_ENCODE_5ntUMI, eCLIP_ENCODE_10ntUMI) are already demultiplexed. Please set demultiplex=False in your configuration.")

# Check for demultiplexing without adapter trimming
if DEMUX == True and TRIM == False:
    print("WARNING: Demultiplexing (demultiplex=True) automatically includes adapter trimming.")
    print("Adapter trimming will be performed during demultiplexing using the specified or default adapter file.")
    print("The adapter_trimming=False setting only affects the separate adapter trimming step for non-multiplexed data.")

# Check for TRIM3 without TRIM compatibility
if TRIM3 == True and TRIM == False:
    raise ValueError("ERROR: 3' trimming (trim3=True) without adapter trimming (adapter_trimming=False) is not implemented. Please set adapter_trimming=True if you want to use trim3=True.")


#############################################
#############################################
### definitions for inputs
#############################################
#############################################

# get multiple inputfiles with or without gz
def get_start_fastqs(wcs):
    # Use the sample-to-file mapping to get the actual file path
    sample_name = wcs.sample if hasattr(wcs, 'sample') else wcs
    if isinstance(sample_name, str):
        return SAMPLE_TO_FILE.get(sample_name, INPATH + f"/{sample_name}.fastq.gz" if GZ else INPATH + f"/{sample_name}.fastq")
    else:
        # Fallback to old behavior for compatibility
        if GZ == True:
            return INPATH+"/{sample}.fastq.gz"
        else:
            return INPATH+"/{sample}.fastq"
   

# get files for adapter trimming
def get_files_for_trim_SE(wcs):
    return config["wdir"]+"/results/barcode_filter/{sample}_renamed.fastq.gz"
  

# get input for header processing
def get_files_for_header_processing(wcs):
    if QUAL_BC == True:
        return config["wdir"]+"/results/barcode_filter/filtered.fastq.gz"
    else:
        return config["infiles"]

# get input for header processing (demultiplexed data)
def get_files_for_header_processing_demux(wcs):
    if QUAL_BC == True:
        return config["wdir"]+"/results/barcode_filter/{sample}_filtered.fastq.gz"
    else:
        return get_start_fastqs(wcs)
  


# get star input
def get_demult_trim_reads(wcs):
    if MIR == True:
        return [config["wdir"]+"/results/mir_analysis/aligned_mir/{sample}.alignMir.non_chimeric.sort.fastq.gz"]
    
    elif MIR != True:
        if TRIM3 != True:
            if DEMUX == True:  # with demulitplexing
                return [config["wdir"]+"/results/demultiplex/flexbarOut_barcode_{sample}.fastq.gz"]
            elif DEMUX != True and TRIM == True:  # without demultiplexing
                return config["wdir"]+"/results/demultiplex/trimmed_{sample}.fastq.gz"
            elif DEMUX != True and TRIM != True:  # without demultiplexing, without trimming
                return config["wdir"]+"/results/barcode_filter/{sample}_renamed.fastq.gz"
        elif TRIM3 == True:
            return config["wdir"]+"/results/trim3/trim3_{sample}.fastq.gz"
                
        
# get reads for 3' trimming
def get_demult_trim_reads_for_3trim(wcs):
    if DEMUX == True:  # with demulitplexing
        return [config["wdir"]+"/results/demultiplex/flexbarOut_barcode_{sample}.fastq.gz"]
    elif DEMUX != True and TRIM == True:  # without demultiplexing
        return config["wdir"]+"/results/demultiplex/trimmed_{sample}.fastq.gz"
    elif DEMUX != True and TRIM != True:  # without demultiplexing, without trimming
        return config["wdir"]+"/results/barcode_filter/{sample}_renamed.fastq.gz"
        


def get_demult_trim_reads_for_qc (wcs):
    if MIR == True:
        return [config["wdir"]+"/results/mir_analysis/aligned_mir/{sample}.alignMir.non_chimeric.sort.fastq.gz"]
    if MIR != True:
        if DEMUX == True:  # with demulitplexing
            return [config["wdir"]+"/results/demultiplex/flexbarOut_barcode_{sample}.fastq.gz"]
        elif DEMUX != True and TRIM == True:  # without demultiplexing
                return config["wdir"]+"/results/demultiplex/trimmed_{sample}.fastq.gz"
        elif DEMUX != True and TRIM != True:  # without demultiplexing, without trimming
            return config["wdir"]+"/results/barcode_filter/{sample}_renamed.fastq.gz"
        
def get_demult_trim_reads_for_mir(wcs):
    if MIR == True:
        if DEMUX == True:  # with demulitplexing
            return [config["wdir"]+"/results/demultiplex/flexbarOut_barcode_{sample}.fastq.gz"]

        if DEMUX != True:  # without demultiplexing
            # if TRIM2 == True:
            #     return config["wdir"]+"/results/demultiplex/trimmed2_{sample}.fastq.gz"
            # else:
            return config["wdir"]+"/results/demultiplex/trimmed_{sample}.fastq.gz"
    else:
        return ""
        

def get_bam_files(wcs):
        return [config["wdir"]+"/results/aligned/{sample}.Aligned.sortedByCoord.out.bam"]


def get_bai_files(wcs):
        return [config["wdir"]+"/results/aligned/{sample}.Aligned.sortedByCoord.out.bam.bai"]

    
def get_bam_dedup():
    if DEDUP == True:
        return config["wdir"]+"/results/aligned/{sample}.Aligned.sortedByCoord.out.duprm.bam"  # Removed .sort - use coordinate-sorted output
    else:
        return config["wdir"]+"/results/aligned/{sample}.Aligned.sortedByCoord.out.bam"

# get read length for STAR    
def mqc_for_readlength():
    if DEMUX != True and TRIM != True:  # without demultiplexing, without trimming
        return config["wdir"]+"/results/fastqc/raw/multiqc_data/multiqc_fastqc.txt"
    else:
        return config["wdir"]+"/results/fastqc/samples_after_demultiplexing_or_trimming/multiqc_data/multiqc_fastqc.txt"
    
def get_star_index():
    if STAR_INDEX is not None:
        chkpnt_file = config["wdir"]+"/results/tmp/.star_index.chkpnt"
        # Create empty checkpoint file
        with open(chkpnt_file, 'w') as f:
            pass
        return STAR_INDEX
    else:
        return re.sub(r"\.[^.]+$", "", config["gtf"]) + "_idx/"
    
def get_star_index_chpnt():
    if STAR_INDEX is not None:
        return config["wdir"]+"/results/tmp/.star_index.chkpnt"
    else:
        return re.sub(r"\.[^.]+$", "", config["gtf"]) + "_idx.chpnt"





#################################################
#################################################
# rule crosslinks, peakcalling and all
# --> conditional output to define which steps should be done
#################################################
#################################################

myoutput = list()

if DEMUX == True:
    # fastqc af raw file
    myoutput.append(expand("{wdir}/results/tmp/.fastqc.chkpnt", wdir=WDIR))
    # barcode filter will be always done when demultiplexing
    myoutput.append(expand("{wdir}/results/barcode_filter/barcodes_detected.txt", wdir=WDIR))
    myoutput.append(expand("{wdir}/results/barcode_filter/renamed.fastq.gz", wdir=WDIR))

# when not demultiplexing do fastqc of all input files
if DEMUX != True:
    myoutput.append(expand("{wdir}/results/fastqc/raw/multiqc_report.html", wdir=WDIR))

# non multiplexed data can be filtered for umi quality optionally
if DEMUX != True and QUAL_BC == True:
    myoutput.append(expand("{wdir}/results/fastqc/filtered/multiqc_report.html", wdir=WDIR))
    myoutput.append(expand("{wdir}/results/barcode_filter/{sample}_renamed.fastq.gz", wdir=WDIR, sample = SAMPLES))

# non multiplexed data can be filtered for umi quality optionally

# adapter adapter_trimming
if DEMUX == True and TRIM == True:  # with demulitplexing
    myoutput.append(expand("{wdir}/results/demultiplex/flexbarOut_barcode_{sample}.fastq.gz",  wdir=WDIR, sample = SAMPLES))
    myoutput.append(expand("{wdir}/results/fastqc/samples_after_demultiplexing_or_trimming/multiqc_report.html", wdir=WDIR),)  # fastqc of trimmed samples
if DEMUX != True and TRIM == True:  # without demultiplexing
        myoutput.append(expand("{wdir}/results/demultiplex/trimmed_{sample}.fastq.gz",  wdir=WDIR, sample = SAMPLES))
        myoutput.append(expand("{wdir}/results/fastqc/samples_after_demultiplexing_or_trimming/multiqc_report.html", wdir=WDIR),)  # fastqc of trimmed samples


# alignment
#myoutput.append(expand("{wdir}/results/aligned/{sample}.Aligned.sortedByCoord.out.bam", sample=SAMPLES, wdir=WDIR))


# deduplication
if DEDUP == True:  # without demultiplexing
        myoutput.append(expand("{wdir}/results/aligned/{sample}.Aligned.sortedByCoord.out.duprm.bam", sample=SAMPLES, wdir=WDIR))

# merge multiple samples
if len(SAMPLES) > 1:
    myoutput.append(expand("{wdir}/results/bam_merged/{groups}.sort.bam",  groups=GROUPS, wdir=WDIR))
    myoutput.append(expand("{wdir}/results/bed_merged/{groups}.minus.bedGraph", groups=GROUPS, wdir=WDIR))
    myoutput.append(expand("{wdir}/results/tmp/.merged.{groups}.bai.chkpnt", groups=GROUPS, wdir=WDIR))

# add miR steps
if MIR == True:
    myoutput.append(expand("{wdir}/results/mir_analysis/aligned_mir/{sample}.alignMir.sam",  wdir=WDIR, sample = SAMPLES))
    myoutput.append(expand("{wdir}/results/mir_analysis/unaligned_target_RNAs/merged_fastq/{sample}.chim.trim.fastq.gz", wdir=WDIR, sample = SAMPLES))
    myoutput.append(expand("{wdir}/results/mir_analysis/aligned_chimeric_bam/chimeric_{sample}.Aligned.sortedByCoord.out.bam", wdir=WDIR, sample = SAMPLES))
    myoutput.append(expand("{wdir}/results/mir_analysis/aligned_chimeric_bam/chimeric_{sample}.Aligned.sortedByCoord.out.bam.bai",  wdir=WDIR, sample = SAMPLES))
    # COMMENTED OUT - No longer generating .sort.bam files as they're unnecessary
    # myoutput.append(expand("{wdir}/results/mir_analysis/aligned_chimeric_bam/chimeric_{sample}.Aligned.sortedByCoord.out.duprm.sort.bam",  wdir=WDIR, sample = SAMPLES))
    myoutput.append(expand("{wdir}/results/mir_analysis/crosslinks/chimeric_{sample}.Aligned.sortedByCoord.out.duprm.named.1nt.minus.bw", wdir=WDIR,  sample = SAMPLES ))
    myoutput.append(expand("{wdir}/results/mir_analysis/crosslinks_merged/chimeric_{groups}.minus.bw", groups=GROUPS, wdir=WDIR))
    myoutput.append(config["wdir"]+"/results/Report_miR.html")
   

rule all:
    input:
        myoutput,
        # # # # crosslink_sites
        expand("{wdir}/results/bw/{sample}.Aligned.sortedByCoord.out.duprm.minus.bw", sample=SAMPLES, wdir=WDIR),
        expand("{wdir}/results/peaks/pureclip_sites_{groups}.bed", groups=GROUPS, wdir=WDIR),
        # # # # FastQ Screen (if enabled)
        (config["wdir"]+"/results/fastqscreen_multi") if FASTQ_SCREEN else [],
        # # # # Report
        config["wdir"]+"/results/Report.html",
        # config["wdir"]+"/results/.finished"  
    message:  "========================= \n ================================ \n RACOON-CLIP FINISHED! \n ================================ \n ================================ " 
   

rule all_crosslinks:
    input:
        myoutput,
        # # # # crosslink_sites
        expand("{wdir}/results/bw/{sample}.Aligned.sortedByCoord.out.duprm.minus.bw", sample=SAMPLES, wdir=WDIR),
        # merged, indexed bam files
        # # # # FastQ Screen (if enabled)
        (config["wdir"]+"/results/fastqscreen_multi") if FASTQ_SCREEN else [],
        # # # # Report
        config["wdir"]+"/results/Report.html",
        # config["wdir"]+"/results/.finished"  
    message:  "========================= \n ================================ \n RACOON-CLIP crosslinks FINISHED! \n ================================ \n ================================ " 
        


#################################################
#################################################
# rules
#################################################
#################################################


###-----------------------------------------
###-----------------------------------------
### Rules for muliplexed input
###-----------------------------------------
###-----------------------------------------

######################
# barcode filter (for muliplexed files)
######################

# select barcodes with sufficient quality
#=======================================

rule high_quality_barcodes:
    input:
        fasta=config["infiles"]
    output:
        "{wdir}/results/tmp/data_qualFilteredIDs.list"
    params:
        barcodeLength=get_barcode_experiment_info()["total_barcode_len"], 
        minBaseQuality=config["minBaseQuality"], 
        seq_format=config["seq_format"]
    threads: 1 # gunzip breaks file with more then one thread
    conda:
        "envs/racoon_main_v0.4.yml"
    message: 
        "========================= \n Extracting barcodes for quality filter \n ================================ \n" 
    shell:
        "zcat -f < {input.fasta} | fastx_trimmer {params.seq_format} -l {params.barcodeLength} | fastq_quality_filter {params.seq_format} -q {params.minBaseQuality} -p 100 | awk 'FNR%4==1 {{ print $1 }}' | sed 's/^@//' > {output}"


# filter fasta for selected barcodes
#===================================

rule filter_barcode_quality:
    input:
        list=expand("{wdir}/results/tmp/data_qualFilteredIDs.list", wdir=WDIR), fastq=config["infiles"]
    output:
        file="{wdir}/results/barcode_filter/filtered.fastq.gz"
    conda:
        "envs/racoon_seqkit_v0.2.yml"
    message: 
        "========================= \n Selecting barcodes over quality filter cutoff \n ================================ \n" 
    threads: 1 # gunzip breaks file with more then one thread
    shell:
        #"bash workflow/scripts/filter_barcode_quality.sh {input.fastq} {input.list} && touch {output.token}"
        """
        if [[ {input.fastq} == *.gz ]]; then
            seqkit grep -f {input.list} {input.fastq} | gzip > {output.file}
        else
            seqkit grep -f {input.list} {input.fastq} | gzip > {output.file}
        fi
        """


rule process_headers:
    input:
        fastq=get_files_for_header_processing
    output:
        file="{wdir}/results/barcode_filter/renamed.fastq.gz"
    message: 
        "========================= \n Processing FASTQ headers \n ================================ \n" 
    threads: 1
    shell:
        """
        if [[ {input.fastq} == *.gz ]]; then
            zcat {input.fastq} | awk '{{if(FNR%4==1){{gsub(" |/", "#", $0)}} print }}' | gzip > {output.file}
        else
            cat {input.fastq} | awk '{{if(FNR%4==1){{gsub(" |/", "#", $0)}} print }}' | gzip > {output.file}
        fi
        """


# write barcode file for barcodes stats
#===================================

rule extract_barcodes:
# this rule writes a txt with all barcodes to be able to do more analysis later
# probably Mirkos code wich directly takes out interesting parameters is faster --> writes a smaller file
    input:
        expand("{wdir}/results/barcode_filter/renamed.fastq.gz", wdir=WDIR)
    output:
        "{wdir}/results/barcode_filter/barcodes_detected.txt"
    params:
        umi1_len=get_barcode_experiment_info()["umi1_len"], 
        total_barcode_len=get_barcode_experiment_info()["total_barcode_len"]
    threads: 1 # gunzip breaks file with more then one thread
    message: 
        "========================= \n Selecting reads with barcode over quality filter cutoff \n ================================ \n" 
    shell:
        """
        zcat {input} | \
        awk -v umi1_len={params.umi1_len} -v exp_bc_len={params.total_barcode_len} '{{ if (FNR%4==2) print substr($1,(umi1_len+1),exp_bc_len) }}' | \
        sort | \
        uniq -c | \
        sort -k1,1rn > {output}
        """


#####################
# demultiplexing
####################

rule demultiplex_flexbar:
# revisit flexbar params some different for mirko
    input:
        fasta=expand("{wdir}/results/barcode_filter/renamed.fastq.gz", wdir=WDIR), 
        barcodes=config["barcodes_fasta"]
    output:
         expand("{wdir}/results/demultiplex/flexbarOut_barcode_{sample}.fastq.gz", sample = SAMPLES, wdir=WDIR )
    params:
        minReadLength=config["min_read_length"], 
        dir=expand("{wdir}/results/demultiplex/", wdir=WDIR),
        adapter=config["adapter_file"],
        barcodeLength=get_barcode_experiment_info()["barcodeLength"],
        filename=config["wdir"]+"/results/demultiplex/flexbarOut"

    threads: workflow.cores  # allows
    conda:
        "envs/racoon_main_v0.4.yml"
    message: 
        "========================= \n Demultiplexing \n ================================  \n adapters, barcodes and UMIs are trimmed off \n provided barcodes: {input.barcodes} \n provided adapters: {params.adapter}" 
    shell:
        """
         mkdir -p {params.dir} && \
        chmod -R +x {params.dir} && \
        flexbar -r {input.fasta} \
        --zip-output GZ \
        --threads {threads} \
        --barcodes {input.barcodes} \
        --barcode-unassigned \
        --barcode-trim-end LTAIL \
        --barcode-error-rate 0 \
        --adapters {params.adapter} \
        --adapter-trim-end RIGHT \
        --adapter-error-rate 0.1 \
        --adapter-min-overlap 1 \
        --min-read-length {params.minReadLength} \
        --umi-tags \
        -t {params.filename}
        """



###-----------------------------------------
###-----------------------------------------
### Rules for demuliplexed input
###-----------------------------------------
###-----------------------------------------

######################
# encode data: move umi
######################
# rule encode_umi:
#     input: 
#       get_start_fastqs
#     output:
#       config["wdir"]+"/results/encUMI/{sample}.fastq.gz" ##!!TODO
#     params:
#         umi_length=get_barcode_experiment_info()["umi1_len"],
#     message: 
#         "========================= \n Handeling encode UMI position \n ================================ \n provided length of UMI: {params.umi_length}"
#     shell:
#         """
#         zcat -f < {input} | awk 'BEGIN{{FS=" "}} NR%4==1 {{print "@" substr($1, ({params.umi_length}+3), 500) "_" substr($1, 2, {params.umi_length})}}; NR%4==2 {{print}} ; NR%4==3 {{print}}; NR%4==0 {{print}};' | gzip > {output}
#         """
#  # " " $2




######################
# umi/barcode filter (for already demultiplexed files)
######################
# if no barcode is in the umi region set barcode to 0
# eCLIP: barcode in second strand?

# select barcodes with sufficient quality
#=======================================
out_high_quality_umi = list()

out_high_quality_umi.append(config["wdir"]+"/results/tmp/{sample}_data_qualFilteredIDs.list")

rule high_quality_umi:
    input:
        fasta=get_start_fastqs(wcs = SAMPLES)
    output:
        out_high_quality_umi
    params:
        barcodeLength=get_barcode_experiment_info()["total_barcode_len"], 
        minBaseQuality=config["minBaseQuality"], 
        seq_format=config["seq_format"]
    threads: 1 # gunzip breaks file with more then one thread
    message: 
        "========================= \n Extracting barcodes for quality filter for {wildcards.sample} \n ================================ \n" 
    conda:
        "envs/racoon_main_v0.4.yml"
    shell:
        """
        zcat -f < {input.fasta} | fastx_trimmer {params.seq_format} -l {params.barcodeLength} | fastq_quality_filter {params.seq_format} -q {params.minBaseQuality} -p 100 | awk 'FNR%4==1 {{ print $1 }}' | sed 's/^@//' > {output}
        """


# filter fasta for selected barcodes
#===================================

rule filter_umi_quality:
    input:
        list=out_high_quality_umi,
        fastq=get_start_fastqs
    output:
        file=config["wdir"]+"/results/barcode_filter/{sample}_filtered.fastq.gz"
    conda:
        "envs/racoon_seqkit_v0.2.yml"
    threads: 1 # gunzip breaks file with more then one thread
    message: 
        "========================= \n Selecting reads with barcode over quality filter cutoff for {wildcards.sample} \n ================================ \n" 
    shell:
        #"bash workflow/scripts/filter_barcode_quality.sh {input.fastq} {input.list} && touch {output.token}"
        """ 
        if [[ {input.fastq} == *.gz ]]; then
            seqkit grep -f {input.list} {input.fastq} | gzip > {output.file}
        else
            seqkit grep -f {input.list} {input.fastq} | gzip > {output.file}
        fi
        """


rule process_headers_demux:
    input:
        fastq=get_files_for_header_processing_demux
    output:
        file=config["wdir"]+"/results/barcode_filter/{sample}_renamed.fastq.gz"
    message: 
        "========================= \n Processing FASTQ headers for {wildcards.sample} \n ================================ \n" 
    threads: 1
    shell:
        """
        if [[ {input.fastq} == *.gz ]]; then
            zcat {input.fastq} | awk '{{if(FNR%4==1){{gsub(" |/", "#", $0)}} print }}' | gzip > {output.file}
        else
            cat {input.fastq} | awk '{{if(FNR%4==1){{gsub(" |/", "#", $0)}} print }}' | gzip > {output.file}
        fi
        """



###-----------------------------------------
#------------------------------
# Adapter trimming
#-------------------------------
###-----------------------------------------
# Adaptor trimming for non multiplexed files


rule write_barcode_file:
    input: 
    output: 
        chkpnt = touch(config["wdir"]+"/results/tmp/.barcode.chkpnt"),
        barcode = config["wdir"]+"/results/demultiplex/barcode.fa"
    params:
        dir = expand("{wdir}/results/demultiplex/", wdir=WDIR),
        barcode=config["barcodes_fasta"],
        barcodeLength=get_barcode_experiment_info()["total_barcode_len"],
        sample=SAMPLES,
        dir_script= SNAKE_PATH
    message: 
        "========================= \n Initialise barcodes + UMIs \n ================================ \n" 
 # Explanation: for encode data or data without bacrode and UMI no barcode file is needed. In both cases do nothing.
 #  if barcodes exist the user should specify them in a fa file. In this case copy the user input barcode fiel to the right location.
 # If there is only a UMI but no barcode, write a barcode file consisting of Ns in the length of the UMI
    shell:
        """
        if [[ "{params.barcodeLength}" == 0 ]]
        then
            touch {output.barcode}
        else
            mkdir -p {params.dir} && \
            chmod -R +x {params.dir} && \
            cd {params.dir} && \
            bc_file={params.barcode} && \
            if [[ -z "$bc_file" ]] 
            then
                chmod +x {params.dir_script}/workflow/scripts/rep_char.sh
                bc=`{params.dir_script}/workflow/scripts/rep_char.sh {params.barcodeLength} "N"`
                printf ">"bc"\n $bc \n"  >> "{output.barcode}"
            else 
                cp {params.barcode} {output.barcode}
            fi
        fi
        """



rule adapter_trimming_flexbar_single_end:
# uses flexbar to cut of adapters on the right
# and UMIs on the left, for Encode data other solution needed because UMI not in anymore!
    input:
        fasta = get_files_for_trim_SE,
        chkpnt_barcode = config["wdir"]+"/results/tmp/.barcode.chkpnt"
        
    output:
         file= config["wdir"]+"/results/demultiplex/trimmed_{sample}.fastq.gz"
    params:
        filename= config["wdir"]+"/results/demultiplex/trimmed_{sample}",
        minReadLength=config["min_read_length"],
        dir=expand("{wdir}/results/demultiplex/", wdir=WDIR),
        adapter=config["adapter_file"],
        barcodeLength=get_barcode_experiment_info()["total_barcode_len"],
        sample="{sample}",
        adapterCycles=config["adapter_cycles"],
        barcode = config["wdir"]+"/results/demultiplex/barcode.fa",
    threads: workflow.cores  # allows
    conda:
        "envs/racoon_main_v0.4.yml"
    message: 
        "========================= \n Adapter trimming for {wildcards.sample} \n ================================ \n barcodes and UMIs are also trimmed off, if present \n provided barcodes: {params.barcode} \n provided adapters: {params.adapter}"  
    shell:
        """
        if [[ "{params.barcodeLength}" != 0 ]]; then
            flexbar -r {input.fasta} \
            --zip-output GZ \
            --threads {threads} \
            --barcodes {params.barcode} \
            --barcode-unassigned \
            --barcode-trim-end LTAIL \
            --barcode-error-rate 0 \
            --adapters {params.adapter} \
            --adapter-trim-end RIGHT \
            --adapter-error-rate 0.1 \
            --adapter-min-overlap 1 \
            --adapter-cycles {params.adapterCycles} \
            --min-read-length {params.minReadLength} \
            --umi-tags \
            -t {params.filename} && \
            cd {params.dir} 
            if [[ -f "trimmed_{params.sample}_barcode_{params.sample}.fastq.gz" ]]; then
                mv "trimmed_{params.sample}_barcode_{params.sample}.fastq.gz" "trimmed_{params.sample}.fastq.gz" 
            fi
            if [[ -f "trimmed_{params.sample}_barcode_bc.fastq.gz" ]]; then
                mv "trimmed_{params.sample}_barcode_bc.fastq.gz" "trimmed_{params.sample}.fastq.gz" 
            fi
        else
            flexbar -r {input.fasta} \
            --zip-output GZ \
            --threads {threads} \
            --adapters {params.adapter} \
            --adapter-trim-end RIGHT \
            --adapter-error-rate 0.1 \
            --adapter-min-overlap 1 \
            --adapter-cycles {params.adapterCycles} \
            --min-read-length {params.minReadLength} \
            -t {params.filename}
        fi
        """

# additional trimming at 3' end after adapter trimming 
# --> 3nt needed for iCLIP3

rule trim3_flexbar:
# revisit flexbar params some different for mirko
    input:
        fasta=get_demult_trim_reads_for_3trim, 
    output:
         config["wdir"]+"/results/trim3/trim3_{sample}.fastq.gz"
    params:
        minReadLength=config["min_read_length"], 
        dir=expand("{wdir}/results/trim3/", wdir=WDIR),
        trim3_length=config["trim3_len"],
        filename=config["wdir"]+"/results/trim3/trim3_{sample}"

    threads: workflow.cores  # allows
    conda:
        "envs/racoon_main_v0.4.yml"
    message: 
        "========================= \n Trimming from 3' end \n ================================  \n {params.trim3_length}nt are trimmed from 3' end \n provided reads: {input.fasta}" 
    shell:
        """
         mkdir -p {params.dir} && \
        chmod -R +x {params.dir} && \
        flexbar -r {input.fasta} \
        --zip-output GZ \
        --threads {threads} \
        -y {params.trim3_length} \
        --min-read-length {params.minReadLength} \
        -t {params.filename}
        """


###-----------------------------------------
###-----------------------------------------
### Alignment, deduplication and crosslink definition (all types of input)
###-----------------------------------------
###-----------------------------------------


# rule get_read_length:
#     input: 
#         ckpnt = config["wdir"]+"/results/fastqc/samples_after_demultiplexing_or_trimming/multiqc_report.html"
#     output: 
#         txt = "params.dir/read_length.txt",
#         ckpnt = touch(config["wdir"]+"/results/tmp/read_length.chkpnt")
#     params:
#         mqc_file = mqc_for_readlength(),
#     conda:
#         "workflow/envs/racoon_main_v0\.3.yml"
#     script:
#         "workflow/scripts/get_read_length.py"

# def get_read_length(path):
#     with open(path) as f:
#         r_l = f.readline()
#         r_l = float(r_l)
#         return r_l


rule create_STAR_index:
    input:
        gtf=config["gtf"],
        genome_fasta=config["genome_fasta"],
    output: 
        idx = directory(re.sub(r"\.[^.]+$", "", config["gtf"]) + "_idx/"),
        chkpnt = touch(re.sub(r"\.[^.]+$", "", config["gtf"]) + "_idx.chpnt")
    conda:
        "envs/racoon_main_v0.4.yml"
    message: 
        "========================= \n Indexing your genome annotation \n ================================ \n provided fasta: {input.genome_fasta} \n provided annotation: {input.gtf} \n" 
    threads: max(workflow.cores - 2, 1) # allows to run barcode rules in parallel
    resources:
        mem_mb = 20000
    shell:
        """  
        STAR --runThreadN {threads} \
        --runMode genomeGenerate \
        --genomeDir {output.idx} \
        --genomeFastaFiles {input.genome_fasta} \
        --readFilesCommand zcat
        """



rule align:
    input:
        reads= get_demult_trim_reads,
        idx = get_star_index(),
        index_chkpnt= get_star_index_chpnt()            
    output:
        chkpnt = touch(config["wdir"]+"/results/.{sample}.bam.SE.chkpnt"),
        bam=config["wdir"]+"/results/aligned/{sample}.Aligned.sortedByCoord.out.bam",

    params:
        gtf=config["gtf"],
        wdir=config["wdir"],
        dir="results/aligned/",
        outFilterMismatchNoverReadLmax=config["outFilterMismatchNoverReadLmax"],
        outFilterMismatchNmax=config["outFilterMismatchNmax"],
        outFilterMultimapNmax=config["outFilterMultimapNmax"],
        sjdbOverhang=int(config["read_length"]) - 1 - int(get_barcode_experiment_info()["total_barcode_len"]), # # readlength -1 - barcodelength - adapter much faster tospecify than to calculated from fastq file
        outReadsUnmapped=config["outReadsUnmapped"],
        outSJfilterReads=config["outSJfilterReads"],
        dedup = DEDUP,
        trim=TRIM
    message: 
            "========================= \n Aligning {wildcards.sample} to genome \n ================================ \n" 
    threads: 4 # x each sample automatically assigned by snakemake
    conda:
        "envs/racoon_main_v0.4.yml"
    resources:
        mem_mb = 40000
        
    shell:
        """
        chmod +x {input.reads} && \
        mkdir -p {params.wdir}/{params.dir} && \
        chmod -R +x {params.wdir}/{params.dir} && \
        cd {params.wdir}/{params.dir} && \
            
        if [[ {input.reads} =~ \.gz$ ]]
        then
            echo "gzip"
            STAR --runMode alignReads \
            --genomeDir {input.idx} \
            --outFileNamePrefix {wildcards.sample}. \
            --outFilterMismatchNoverReadLmax {params.outFilterMismatchNoverReadLmax} \
            --outFilterMismatchNmax {params.outFilterMismatchNmax} \
            --outFilterMultimapNmax {params.outFilterMultimapNmax} \
            --outSAMattributes All \
            --alignEndsType "Extend5pOfRead1" \
            --sjdbGTFfile {params.gtf} \
            --sjdbOverhang {params.sjdbOverhang} \
            --outReadsUnmapped {params.outReadsUnmapped} \
            --outSJfilterReads {params.outSJfilterReads} \
            --readFilesCommand zcat \
            --outSAMtype BAM SortedByCoordinate \
            --readFilesIn {input.reads} \
            --runThreadN {threads}
        else
            STAR --runMode alignReads \
            --genomeDir {input.idx} \
            --outFileNamePrefix {wildcards.sample}. \
            --outFilterMismatchNoverReadLmax {params.outFilterMismatchNoverReadLmax} \
            --outFilterMismatchNmax {params.outFilterMismatchNmax} \
            --outFilterMultimapNmax {params.outFilterMultimapNmax} \
            --alignEndsType "Extend5pOfRead1" \
            --sjdbGTFfile {params.gtf} \
            --sjdbOverhang {params.sjdbOverhang} \
            --outSAMattributes All \
            --outReadsUnmapped {params.outReadsUnmapped} \
            --outSJfilterReads {params.outSJfilterReads} \
            --outSAMtype BAM SortedByCoordinate \
            --readFilesIn {input.reads} \
            --runThreadN {threads}
        fi
        """


rule bam_index:
    input:
        bam = get_bam_files
    output:
         chpnt = touch(config["wdir"]+"/results/tmp/.{sample}.bai.chkpnt")
    # params:
    #     files = config["wdir"]+"/results/aligned/{sample}.Aligned.sortedByCoord.out.bam"
    conda:
        "envs/racoon_samtools.yml"
    message: 
            "========================= \n Indexing .bam file of {wildcards.sample} \n ================================ \n" 
    threads: 1 # x each sample automatically assigned by snakemake
    shell:
        """
        samtools index {input.bam}
        """


#####################
# deduplication
#####################

rule deduplication:
    input:
        bam = get_bam_files,
        chpnt_bai = config["wdir"]+"/results/tmp/.{sample}.bai.chkpnt"
    output:
        bam="{wdir}/results/aligned/{sample}.Aligned.sortedByCoord.out.duprm.bam"
    params:
        bam = config["wdir"]+"/results/aligned/{sample}.Aligned.sortedByCoord.out.bam",
        dedup = DEDUP,
        log="{wdir}/results/aligned/{sample}.Aligned.sortedByCoord.out.duprm.log"   
    conda:
        "envs/racoon_umi_tools_v0.3.yml"
    message: 
            "========================= \n Deduplicating {wildcards.sample} \n ================================ \n" 
    threads: 1 # x each sample automatically assigned by snakemake
    shell:
        """
        if [[ {params.dedup} == True ]]
        then
            umi_tools dedup -I {input.bam} -L {params.log} -S {output.bam} --extract-umi-method read_id --method unique
        else 
            mv {input.bam} {output.bam}
        fi
        """


# COMMENTED OUT - sort_bams rule is unnecessary for bamtobed and causes issues with samtools merge
# rule sort_bams:
#     input:
#         config["wdir"]+"/results/aligned/{sample}.Aligned.sortedByCoord.out.duprm.bam"
#     output:
#         config["wdir"]+"/results/aligned/{sample}.Aligned.sortedByCoord.out.duprm.sort.bam"
#     conda:
#         "envs/racoon_main_v0.4.yml"
#     threads: 1 # x each sample automatically assigned by snakemake
#     message: 
#             "========================= \n Sorting deduplicated {wildcards.sample} \n ================================ \n" 
#     shell:
#         "samtools sort -n {input} -o {output}"



######################
# extraction of crosslinked nucleotides
#######################
rule make_genome_index:
    params:
        genome_fasta=config["genome_fasta"]
    output:
        config["genome_fasta"]+".fai"
    conda:
        "envs/racoon_samtools.yml"
    threads: workflow.cores * 0.5 # so it can run parrallel to deduplication and bam sorting
    message: 
            "========================= \n Indexing genome fasta \n ================================ \n provided genome: {params.genome_fasta}" 
    shell:
        "samtools faidx {params.genome_fasta}"




rule get_crosslinks:
    input:
        genome_idx=config["genome_fasta"]+".fai",
        bam= get_bam_dedup(),
        chkpnt=config["wdir"]+"/results/tmp/.{sample}.bai.chkpnt"
    output:
        bed="{wdir}/results/tmp/{sample}.Aligned.sortedByCoord.out.duprm.bed",
        bed_shift="{wdir}/results/bed/{sample}.Aligned.sortedByCoord.out.duprm.shifted.bed",
        bed_plus="{wdir}/results/bed/{sample}.Aligned.sortedByCoord.out.duprm.plus.bed",
        bed_minus="{wdir}/results/bed/{sample}.Aligned.sortedByCoord.out.duprm.minus.bed",

    params:
        genome_fasta=config["genome_fasta"],
        wdir=config["wdir"]
    conda:
        "envs/racoon_main_v0.4.yml"
    message: 
            "========================= \n Obtaining crosslinked nucleotide {wildcards.sample} \n ================================ \n" 
    shell:
        """
        #### Convert all read locations to intervals in bed file format using BEDTools
        bedtools bamtobed -i {input.bam} > {output.bed}
        
        #### Filter out reads at chromosome boundaries (first nt on + strand, last nt on - strand)
        awk 'BEGIN{{OFS="\t"}} FNR==NR{{chr_len[$1]=$2; next}} !(($6=="+" && $2==0) || ($6=="-" && $3==chr_len[$1]))' {params.genome_fasta}.fai {output.bed} > {output.bed}.tmp && mv {output.bed}.tmp {output.bed}
        
        #### Shift intervals depending on the strand by 1 bp upstream using BEDTools
        bedtools shift -m 1 -p -1 -i {output.bed} -g {params.genome_fasta}.fai | sort -k1,1 -k2,2n -k3,3n > {output.bed_shift}
        
        #### Extract the 5' end of the shifted intervals and pile up into coverage track in bedgraph file format (separately for each strand) using BEDTools (in case of RPM-normalised coverage tracks, use additional parameter -scale with 1,000,000/#mappedReads)
        bedtools genomecov -bg -strand + -5 -i {output.bed_shift} -g {params.genome_fasta}.fai | LC_COLLATE=C sort -k1,1 -k2,2n -k3,3n > {output.bed_plus}
        bedtools genomecov -bg -strand - -5 -i {output.bed_shift} -g {params.genome_fasta}.fai | LC_COLLATE=C sort -k1,1 -k2,2n -k3,3n > {output.bed_minus}
        """

rule turn_crosslinks_to_bw:
    input:
        bed_plus="{wdir}/results/bed/{sample}.Aligned.sortedByCoord.out.duprm.plus.bed",
        bed_minus="{wdir}/results/bed/{sample}.Aligned.sortedByCoord.out.duprm.minus.bed",
    output:
        bw_plus="{wdir}/results/bw/{sample}.Aligned.sortedByCoord.out.duprm.plus.bw",
        bw_minus="{wdir}/results/bw/{sample}.Aligned.sortedByCoord.out.duprm.minus.bw"
    params:
        genome_fasta=config["genome_fasta"],
    conda:
        "envs/racoon_main_v0.4.yml"
    shell:
        """
        #### conversion of bedgraph files to bw file format files using bedGraphToBigWig of the kentUtils suite
        bedGraphToBigWig {input.bed_plus} {params.genome_fasta}.fai {output.bw_plus} 
        bedGraphToBigWig {input.bed_minus} {params.genome_fasta}.fai {output.bw_minus}
        """



##############################
# merge bw
##############################


rule write_group_file:
    input:
    output:
        config["wdir"]+"/results/tmp/groups/{groups}.txt"
    params:
        dir=config["wdir"]+"/results/tmp/groups/",
        groups=config["experiment_groups"],
        group_file=config["experiment_group_file"],
        samples=SAMPLES
    message: 
        "========================= \n Detecting sample groups \n ================================ \n" 
    shell:
        """
        mkdir -p {params.dir} 
        chmod -R +x {params.dir} 
        group_file={params.group_file} 
        if [ -z "$group_file" ] 
        then
            echo "No groups specified. I will merge all samples for you."
            cd {params.dir} 
            for s in {params.samples}; do
                printf "all_samples $s \n" >> {output}
            done
        else 
            chmod +x {params.group_file} 
            cd {params.dir}
            awk '{{print $0 > $1".txt"}}' {params.group_file}
        fi
        """


# rule experiment_groups:
#     input:
#         config["wdir"]+"/results/tmp/group_file.txt"
#     output:
#         config['wdir']+ "/results/groups/{groups}.txt"
#     params: dir_groups= config['wdir']+ "/results/groups/"
#     shell:
#         """
#         mkdir -p {params.dir_groups} && \
#         chmod +x {params.dir_groups} && \
#         cd {params.dir_groups} && \
#         awk '{{print $0 > $1".txt"}}' {input}
#         """

checkpoint merge_bw:
    input:
        groups = config["wdir"]+"/results/tmp/groups/{groups}.txt",
        bws = expand(config['wdir']+"/results/bw/{sample}.Aligned.sortedByCoord.out.duprm.minus.bw", sample = SAMPLES )
    output:
        bed_p=config['wdir']+"/results/bed_merged/{groups}.plus.bedGraph",
        bed_m=config['wdir']+"/results/bed_merged/{groups}.minus.bedGraph",
        bed_p_sort=config['wdir']+"/results/bed_merged/{groups}.sort.plus.bedGraph",
        bed_m_sort=config['wdir']+"/results/bed_merged/{groups}.sort.minus.bedGraph",
        bw_p=config['wdir']+"/results/bw_merged/{groups}.plus.bw",
        bw_m=config['wdir']+"/results/bw_merged/{groups}.minus.bw"
    params:
        genome_fasta=config["genome_fasta"],
        wdir=config["wdir"]
    conda:
        "envs/racoon_ucsc_v0.3.yml"
    message: 
        "========================= \n Merging crosslink files of group {wildcards.groups} \n ================================ \n" 
    shell:
        """
        ### merge plus files per group
        s="$(awk '{{ print "{params.wdir}/results/bw/" $2 ".Aligned.sortedByCoord.out.duprm.plus.bw"}}' {input.groups} )" 
        chmod +x $s 
        bigWigMerge $s {output.bed_p} 
        LC_COLLATE=C sort -k1,1 -k2,2n {output.bed_p} -o {output.bed_p_sort} 
        bedGraphToBigWig {output.bed_p_sort} {params.genome_fasta}.fai {output.bw_p}
        \
        ### merge minus files per group
        r="$(awk '{{ print "{params.wdir}/results/bw/" $2 ".Aligned.sortedByCoord.out.duprm.minus.bw"}}' {input.groups} )" 
        chmod +x $r 
        bigWigMerge $r {output.bed_m} 
        chmod +x {output.bed_m} 
        LC_COLLATE=C sort -k1,1 -k2,2n {output.bed_m} -o {output.bed_m_sort} 
        bedGraphToBigWig {output.bed_m_sort} {params.genome_fasta}.fai {output.bw_m}
        """



checkpoint merge_bam:
    input:
        groups = config["wdir"]+"/results/tmp/groups/{groups}.txt",
        bam = expand(get_bam_dedup(), sample = SAMPLES)
    output:
        bam_merge = config['wdir']+"/results/tmp/{groups}.bam"
    params: 
        wdir = config['wdir'],
        dedup=DEDUP
    threads: 1
    conda:
        "envs/racoon_samtools.yml"
    message: 
        "========================= \n Merging .bam files of group {wildcards.groups} \n ================================ \n" 
    shell:
        """
        if [[ "{params.dedup}" == "True" ]]; then
            s="$(awk '{{ print "{params.wdir}/results/aligned/" $2 ".Aligned.sortedByCoord.out.duprm.bam"}}' {input.groups} )"  # Removed .sort - use coordinate-sorted files
            samtools merge -f {output.bam_merge} $s
        else
            s="$(awk '{{ print "{params.wdir}/results/aligned/" $2 ".Aligned.sortedByCoord.out.bam"}}' {input.groups} )" 
            samtools merge -f {output.bam_merge} $s
        fi

        """
        
rule sort_merged_bams:
    input:
        config['wdir']+"/results/tmp/{groups}.bam"
    output:
        config['wdir']+"/results/bam_merged/{groups}.sort.bam"
    conda:
        "envs/racoon_samtools.yml"
    threads: 1 # x each sample automatically assigned by snakemake
    message: 
            "========================= \n Sorting merged bam files of group {wildcards.groups} \n ================================ \n" 
    shell:
        "samtools sort {input} -o {output}"
        
rule merged_bam_index:
    input:
        config['wdir']+"/results/bam_merged/{groups}.sort.bam",
    output:
        chpnt = touch(config["wdir"]+"/results/tmp/.merged.{groups}.bai.chkpnt"),
        idx = config['wdir']+"/results/bam_merged/{groups}.sort.bam.bai"
    # params:
    #     files = config["wdir"]+"/results/aligned/{sample}.Aligned.sortedByCoord.out.bam"
    conda:
        "envs/racoon_samtools.yml"
    message: 
            "========================= \n Indexing .bam file of {wildcards.groups} \n ================================ \n" 
    threads: 1 # x each sample automatically assigned by snakemake
    shell:
        """
        samtools index {input}
        """

###-----------------------------------------
###-----------------------------------------
### Peak calling
###-----------------------------------------
###-----------------------------------------

rule pc_peak_calling:
    input:
        bam_merge = config['wdir']+"/results/bam_merged/{groups}.sort.bam",
        bam_index = config['wdir']+"/results/bam_merged/{groups}.sort.bam.bai"
    output:
        peaks = config['wdir']+"/results/peaks/pureclip_sites_{groups}.bed",
    params: 
        fasta=config["genome_fasta"],
    threads: max(workflow.cores - 2, 1) # allows to run barcode rules in parallel
    resources:
        mem_mb = 180000
    conda:
        "envs/racoon_pureclip.yml"
    message: 
        "========================= \n Pureclip peak calling on group {wildcards.groups} \n this may take several days ================================ \n" 
    shell:
        """
        pureclip -i {input.bam_merge} \
        --bai {input.bam_index} \
        -g {params.fasta} \
        -o {output.peaks} \
        -nt {threads} \
        """


###-----------------------------------------
###-----------------------------------------
### Quality controls
###-----------------------------------------
###-----------------------------------------

# 1.1) one raw file
#####################
rule fastqc_multiplexed:
    input:
        config["infiles"]
    output:
        touch("{wdir}/results/tmp/.fastqc.chkpnt"),
    params: wdir=config["wdir"]
    conda:
        "envs/racoon_fastqc.yml"
    threads: 1 # fastqc can use 1 thread per sample
    message: 
        "========================= \n FastQC of {input} \n ================================ \n" 
    shell:
        """
        mkdir -p {params.wdir}/results/fastqc/raw && \
        fastqc {input} -o {params.wdir}/results/fastqc/raw -q 
        """
        
rule fastqc_one_filtered:
    input:
        config["wdir"]+"/results/barcode_filter/filtered.fastq.gz"
    output:
        touch("{wdir}/results/tmp/.fastqc.filtered.chkpnt"),
    params: wdir=config["wdir"]
    conda:
        "envs/racoon_fastqc.yml"
    threads: 1 # fastqc can use 1 thread per sample
    message: 
        "========================= \n FastQC of {input} \n ================================ \n" 
    shell:
        """
        mkdir -p {params.wdir}/results/fastqc/filtered && \
        fastqc {input} -o {params.wdir}/results/fastqc/filtered -q 
        """       

# 1.2) multiple raw files
#####################
rule fastqc_raw_multi:
    input:
        [get_start_fastqs]
    output:
        touch("{wdir}/results/tmp/.fastqc.{sample}.raw.chkpnt"),
    params: wdir=config["wdir"]
    conda:
        "envs/racoon_fastqc.yml"
    threads: 1 # fastqc can use 1 thread per sample
    message: 
        "========================= \n FastQC of {input} \n ================================ \n" 
    shell:
        """
        mkdir -p {params.wdir}/results/fastqc/raw && \
        fastqc {input} -o {params.wdir}/results/fastqc/raw -q 
        """

in_multiqc_raw_multi = list()
in_multiqc_filt_multi = list()
in_multiqc = list()


in_multiqc_raw_multi.append(expand("{wdir}/results/tmp/.fastqc.{sample}.raw.chkpnt", wdir=WDIR, sample=SAMPLES))
in_multiqc_filt_multi.append(expand("{wdir}/results/tmp/.fastqc.filtered.{sample}.chkpnt", wdir=WDIR, sample=SAMPLES))
in_multiqc.append(expand("{wdir}/results/tmp/.fastqc.{sample}.trim.chkpnt", wdir=WDIR, sample=SAMPLES))



rule multiqc_raw_multi:
    input:
        in_multiqc_raw_multi
    output:
        config["wdir"]+"/results/fastqc/raw/multiqc_report.html"
    params: wdir=config["wdir"]
    conda:
       "envs/racoon_fastqc.yml"
    threads: 1
    message: 
        "========================= \n MultiQC of raw read files \n ================================ \n" 
    shell:
        """
        cd {params.wdir}/results/fastqc/raw  && \
        multiqc -f --export .
        """

rule fastqc_filt_multi:
    input:
        get_files_for_trim_SE
    output:
        touch(config["wdir"]+"/results/tmp/.fastqc.filtered.{sample}.chkpnt"),
    params: wdir=config["wdir"]
    conda:
        "envs/racoon_fastqc.yml"
    threads: 1 # fastqc can use 1 thread per sample
    message: 
        "========================= \n FastQC of quality filtered read file of {wildcards.sample} \n ================================ \n" 
    shell:
        """
        mkdir -p {params.wdir}/results/fastqc/filtered && \
        fastqc {input} -o {params.wdir}/results/fastqc/filtered -q 
        """

rule multiqc_filt_multi:
    input:
        in_multiqc_filt_multi
    output:
        config["wdir"]+"/results/fastqc/filtered/multiqc_report.html"
    params: wdir=config["wdir"]
    conda:
       "envs/racoon_fastqc.yml"
    threads: 1
    message: 
        "========================= \n MultiQC of quality filtered read files \n ================================ \n" 
    shell:
        """
        cd {params.wdir}/results/fastqc/filtered  && \
        multiqc -f --export .
        """



# 2) after demultiplexing/adapter trimming
########################

rule fastqc_samples:
    input:
        file= get_demult_trim_reads
    output:
        touch("{wdir}/results/tmp/.fastqc.{sample}.trim.chkpnt"),
    params: wdir=config["wdir"]
        # "results/fastqc/samples_after_demultiplexing_or_trimming/{sample}.html",
        # "results/fastqc/samples_after_demultiplexing_or_trimming/{sample}.zip"
    conda:
        "envs/racoon_fastqc.yml"
    message: 
        "========================= \n FastQC of on adapter trimmed read file of {wildcards.sample} \n ================================ \n" 
    threads: 1
    shell:
        """
        mkdir -p {params.wdir}/results/fastqc/samples_after_demultiplexing_or_trimming/ && \
        fastqc {input.file} -o {params.wdir}/results/fastqc/samples_after_demultiplexing_or_trimming/ -q
        """

rule multiqc:
    input:
        in_multiqc
    output:
        config["wdir"]+"/results/fastqc/samples_after_demultiplexing_or_trimming/multiqc_report.html"
    params: wdir=config["wdir"]
    conda:
        "envs/racoon_fastqc.yml"
    message: 
        "========================= \n MultiQC of on adapter trimmed read files \n ================================ \n" 
    threads: 1
    shell:
        """
        cd {params.wdir}/results/fastqc/samples_after_demultiplexing_or_trimming/  && \
        multiqc -f --export .
        """


###-----------------------------------------
###-----------------------------------------
# make report
###-----------------------------------------
###-----------------------------------------

stats = list()


# raw 
if DEMUX == True:
    stats.append(config["wdir"]+"/results/tmp/.fastqc.chkpnt")  
if DEMUX != True:
    stats.append(config["wdir"]+"/results/fastqc/raw/multiqc_report.html")
    
# quality filter    
if DEMUX == True and QUAL_BC == True:
    stats.append(config["wdir"]+"/results/tmp/.fastqc.filtered.chkpnt")    
if DEMUX != True and QUAL_BC == True:
    stats.append(config["wdir"]+"/results/fastqc/filtered/multiqc_report.html")
    
# demultiplexed 
if DEMUX == True:
    stats.append(config["wdir"]+"/results/fastqc/samples_after_demultiplexing_or_trimming/multiqc_report.html")
    
# trimming    
if DEMUX != True and TRIM == True:
    stats.append(config["wdir"]+"/results/fastqc/samples_after_demultiplexing_or_trimming/multiqc_report.html")

# fastq_screen
if FASTQ_SCREEN == True:
    stats.append(config["wdir"]+"/results/fastqscreen_multi")


    
# if DEDUP == True:
#     stats.append(config["wdir"]+"/results/fastqc/separate_samples_bam_duprm/multiqc_report.html")

rule fastqc_stats:
    input:
        f1=stats        
    output:
        touch(config["wdir"]+"/results/tmp/.fastqc_stats_chkpnt")
    params: 
        wdir=config["wdir"],
        qual_bc=QUAL_BC,
        demux=DEMUX,
        trim=TRIM
    message: 
        "========================= \n Preparing numbers for Report \n ================================ \n" 
    shell:
        """
        unzip -o -q "{params.wdir}/results/fastqc/raw/*.zip" -d {params.wdir}/results/fastqc/raw/ 

        if [[ "{params.qual_bc}" == "True" ]]
        then
            unzip -o -q "{params.wdir}/results/fastqc/filtered/*.zip" -d {params.wdir}/results/fastqc/filtered/ 
        fi
        
        if [[ "{params.demux}" == "True" || "{params.trim}" == "True" ]]
        then
            unzip -o -q "{params.wdir}/results/fastqc/samples_after_demultiplexing_or_trimming/*.zip" -d {params.wdir}/results/fastqc/samples_after_demultiplexing_or_trimming/ 
        fi
        """

rule make_report:
    input:
        #flowchart=config["wdir"]+"/results/dag.svg",
        stats=config["wdir"]+"/results/tmp/.fastqc_stats_chkpnt",
        # conda_prefix=config["wdir"]+"/results/tmp/conda_r_prefix.txt"
    output:
        config["wdir"]+"/results/Report.html"
    params:
        wdir=config["wdir"],
        config= config,
        snake_path= SNAKE_PATH
    log:
        config["wdir"]+"/results/tmp/make_report.log"
    message: 
        "========================= \n Making racoon Report \n ================================ \n" 
    conda:
        "envs/racoon_R_v0.3.yml"
    script:
        "rules/make_report.R"



finished = list()
finished.append(config["wdir"]+"/results/Report.html")


rule cleanup:
    input: 
        finished
    output:
        touch(config["wdir"]+"/results/.finished")
    params:
        tmp_dir=config["wdir"]+"/results/tmp"
    message: 
        "========================= \n Cleaning up temporary files \n ================================ \n" 
    shell:
        """
        rm -r {params.tmp_dir}
        """



###-----------------------------------------
###-----------------------------------------
### FastQ Screen
###-----------------------------------------
###-----------------------------------------


rule fastq_screen:
    input:
        # Use raw FASTQ files (same input as STAR)
        get_demult_trim_reads
    output:
        html=config["wdir"]+"/results/fastqscreen/{sample}_screen.html",
        txt=config["wdir"]+"/results/fastqscreen/{sample}_screen.txt"
    params:
        outdir=config["wdir"]+"/results/fastqscreen",
        config=config["fastqScreen_config"]
    threads: 4
    conda:
        "envs/racoon_fastqscreen.yml"
    log:
        "logs/fastqc_screen/{sample}.log"
    shell:
        """
        mkdir -p {params.outdir}
        fastq_screen --conf {params.config} \
            --outdir {params.outdir} \
            --threads {threads} \
            --aligner bowtie2 \
            --force {input}
        
        # Rename output files to match expected naming pattern
        # fastq_screen names files based on input filename, we need consistent {sample}_screen naming
        input_basename=$(basename {input} .fastq.gz)
        if [ -f "{params.outdir}/${{input_basename}}_screen.html" ]; then
            mv "{params.outdir}/${{input_basename}}_screen.html" {output.html}
        fi
        if [ -f "{params.outdir}/${{input_basename}}_screen.txt" ]; then
            mv "{params.outdir}/${{input_basename}}_screen.txt" {output.txt}
        fi
        if [ -f "{params.outdir}/${{input_basename}}_screen.png" ]; then
            mv "{params.outdir}/${{input_basename}}_screen.png" "{params.outdir}/{wildcards.sample}_screen.png"
        fi
        """

rule fastq_screen_multi:
    input:
        expand(config["wdir"]+"/results/fastqscreen/{sample}_screen.txt", sample=SAMPLES)
    output:
        directory(config["wdir"]+"/results/fastqscreen_multi")
    params:
        outdir=config["wdir"]+"/results/fastqscreen_multi",
        indir=config["wdir"]+"/results/fastqscreen"
    threads: 1
    conda:
        "envs/racoon_fastqscreen.yml"
    shell:
        """
        mkdir -p {params.outdir}
        multiqc -f -o {params.outdir} -n fastqscreen_multiqc.html {params.indir}
        """

###-----------------------------------------
###-----------------------------------------
### miR chimeric Alignment
###-----------------------------------------
###-----------------------------------------

# This is used for mir-eCLIP data
## key word definitions:
## chimeric reads = reads that contain the sequence of both a miR and a gene from the general annotation
## short reads = the first 24nt of the chimeric reads. These should contain the miR part of the chimeric read, otherwise the chimeic read is discarded.
## miRs = short reads that were aligned to the miR annotation

####################
# index miR annotation
####################
# mir annotation is indexed with bowtie2

rule make_miR_index:
    params:
        miR_genome=config["mir_genome_fasta"],
        folder=config["wdir"]+"/results/tmp/mir_index"
    output:
        touch(config["wdir"]+"/results/.miR_index.chkpnt")
    conda:
        "envs/racoon_bowtie2_v0.2.yml"
    threads: workflow.cores * 0.5 # so it can run parallel to deduplication and bam sorting
    shell:
        """
        mkdir -p {params.folder} && \
        chmod +x -R {params.folder} && \
        bowtie2-build {params.miR_genome} {params.folder}/mir_index
        """


####################
# shorten reads
####################
## reads are shortened to the first 24nt of the 5'end for better alignemnt
## in a chimeric read these 24nt contain the mirRNA (21nt lenght)

rule shorten_unaligned:
    input:
        get_demult_trim_reads_for_mir
    output:
        config["wdir"]+"/results/mir_analysis/unaligned_reads_first24nt/{sample}.24nt.fastq"
    conda:
        "envs/racoon_main_v0.4.yml"
    threads: 1
    shell:
        "zcat {input} | fastx_trimmer -l 24  -o {output}"

####################
# align to miR annotation
####################
## the shortened reads are aligned to the miR annotation using bowtie2 aligner
## trim5 2 gave the best alignemt results

rule align_miR:
    input:
        chpnt=config["wdir"]+"/results/.miR_index.chkpnt",
        unaligned=config["wdir"]+"/results/mir_analysis/unaligned_reads_first24nt/{sample}.24nt.fastq"
    params:
        index=config["wdir"]+"/results/tmp/mir_index"
    output:
        sam=config["wdir"]+"/results/mir_analysis/aligned_mir/{sample}.alignMir.sam"
    conda:
        "envs/racoon_bowtie2_v0.2.yml"
    threads: 2
    shell:
        """
        cd {params.index} && \
        bowtie2 -x mir_index \
        -q {input.unaligned} \
        --local \
        -D 20 \
        -R 3 \
        -L 10 \
        -i S,1,0.50 \
        -k 20 \
        --trim5 2 \
        -S {output.sam} \
        --quiet \
        --sam-no-qname-trunc \
        --threads {threads}
        """

######################
# split between read with and without
#####################
## read IDs are split between mapped to mir annotation ( = chimeric reads) and
## not mapped to mir genome ( = non-chimeric reads)

rule split_by_miR_alignment:
    input:
        miR_sam = config["wdir"]+"/results/mir_analysis/aligned_mir/{sample}.alignMir.sam"
    output:
        chimeric = config["wdir"]+"/results/mir_analysis/aligned_mir/{sample}.alignMir.chimeric.bam",
        non_chimeric = config["wdir"]+"/results/tmp/non_chimeric/{sample}.alignMir.non_chimeric.list",
        ckpnt = touch(config["wdir"]+"/results/tmp/{sample}/.{sample}.split_alignmir.chpnt")
    params:
        filename=config["wdir"]+"/results/mir_analysis/aligned_mir/",
        filename_non_chimeric = config["wdir"]+"/results/tmp/non_chimeric/"
    conda:
        "envs/racoon_samtools.yml"
    shell:
        """
        # sam flags: 0 == aligned, 4 == not aligned
        samtools view -f 0 -b {input.miR_sam} > {output.chimeric} && \
        samtools view -f 4 {input.miR_sam} | awk -v FS='\t' -v OFS='\t' '{{ print $1 > "{params.filename_non_chimeric}/{wildcards.sample}.alignMir.non_chimeric.list" }}' 
        """

rule stats_miR_alignment:
    input:
        chimeric = expand(config["wdir"]+"/results/mir_analysis/aligned_mir/{sample}.alignMir.chimeric.bam", sample = SAMPLES),
        non_chimeric = expand(config["wdir"]+"/results/tmp/non_chimeric/{sample}.alignMir.non_chimeric.list", sample = SAMPLES)
    output:
        chimeric_stats = config["wdir"]+"/results/mir_analysis/aligned_mir/chimeric_bowtie_stats.txt",
        non_chimeric_stats = config["wdir"]+"/results/mir_analysis/aligned_mir/non_chimeric_bowtie_stats.txt"
    params:
        filename=config["wdir"]+"/results/mir_analysis/aligned_mir/",
        filename_non_chimeric = config["wdir"]+"/results/tmp/non_chimeric/"
    conda:
        "envs/racoon_samtools.yml"
    shell:
        """
        for i in {input.chimeric}
        do
            samtools view $i | wc -l >> {output.chimeric_stats}
        done

        for i in {input.non_chimeric}
        do
            wc $i -l >> {output.non_chimeric_stats}
        done

        """

########################
# turn fastq into fasta to reduce seqkit memory usage
########################
# rule non_chimeric_fastq:
#     input:
#         non_chimeric_list = config["wdir"]+"/results/tmp/non_chimeric/{sample}.alignMir.non_chimeric.list",
#         fastq = get_demult_trim_reads_for_mir
#     output:
#         fasta = config["wdir"]+"/results/mir_analysis/fasta/{sample}.fasta",
 
#     params:
#     threads:1
#     resources:
#         mem_mb = 40000 # TODO does this work
#     conda:
#         "envs/racoon_main_v0.1.yml"
#     shell:
#         """
#         zcat {input.fastq) | fastq_to_fasta -z -o {output.fasta}
#         """

#####################
# make fastq files of non-chimeric reads
####################
# this fastq file is then passed back to the normal ieCLIP pipeline steps (next step is the alignemnt step)
rule non_chimeric_fastq:
    input:
        non_chimeric_list = config["wdir"]+"/results/tmp/non_chimeric/{sample}.alignMir.non_chimeric.list",
        fastq = get_demult_trim_reads_for_mir
    output:
        fastq = config["wdir"]+"/results/mir_analysis/aligned_mir/{sample}.alignMir.non_chimeric.fastq",
        fastq_sort = config["wdir"]+"/results/mir_analysis/aligned_mir/{sample}.alignMir.non_chimeric.sort.fastq.gz"
    params:
    threads:1
    resources:
        mem_mb = 40000 # TODO does this work
    conda:
        "envs/racoon_seqkit_v0.2.yml"
    shell:
        """
            seqkit grep -n --quiet -f {input.non_chimeric_list} {input.fastq} > {output.fastq} && \
            seqkit sort -n {output.fastq} | gzip > {output.fastq_sort}
        """

#############################
# split aligned miRs by miR start position
#############################
## not all mirR in the 24nt fragment start at the first nt, many start at the 2. or 3. nt
## it is important to threat them differently because the first nucleotide in the protein coding RNA will be at position 22 after the start of the miR not after the read start
## therfore reads are split by read start in two steps (two rules)

## 1) a list is made for each possible read start containing the read IDs with the given read start
## 1.1) a second list stored the name of the miR the read was aligned to

rule split_by_mapping_start:
    input:
        miR_bam = config["wdir"]+"/results/mir_analysis/aligned_mir/{sample}.alignMir.chimeric.bam",
        chkpnt = config["wdir"]+"/results/tmp/{sample}/.{sample}.split_alignmir.chpnt"
    output:
        ckpnt=touch(config["wdir"]+"/results/tmp/{sample}/.{sample}.split_mir.chpnt")
    params:
        #miR_sam_sort = config["wdir"]+"/results/mir_analysis/aligned_mir/{sample}.sort.sam"
        filename=config["wdir"]+"/results/tmp/mir_split/{sample}",
        # list=config["wdir"]+"/results/tmp/mir_split/{sample}/{sample}_startnt{start}.list",
        # miR_names=config["wdir"]+"/results/tmp/mir_split/{sample}/{sample}_startnt{start}.mirNames.txt"
    conda:
        "envs/racoon_samtools.yml"
    shell:
        """
        mkdir -p {params.filename} && \
        chmod +X {params.filename} && \
        # split unaligned reads by position of first mapping
        samtools view {input.miR_bam} | awk -v FS='\t' -v OFS='\t' '{{ print $1 > "{params.filename}/{wildcards.sample}_startnt"$4".list" }}' && \
        samtools view {input.miR_bam} | awk  -v FS='\t' -v OFS='\t' '{{ print $1, $3"_"$1 > "{params.filename}/{wildcards.sample}_startnt"$4".mirNames.txt" }}'
        """



## 2) the long chimeric (previously unaligned) reads are split into serveral files by the start position of the miR
rule chimeric_split:
    input:
        chkpnt = config["wdir"]+"/results/tmp/{sample}/.{sample}.split_mir.chpnt",
        fastq = get_demult_trim_reads_for_mir
    output:
        fastq = touch(config["wdir"]+"/results/tmp/mir_split/{sample}/{sample}_startnt{start}.fastq"),
        fastq_named = touch(config["wdir"]+"/results/tmp/mir_split/{sample}/{sample}_startnt{start}.named.fastq"),
        fastq_sort = touch(config["wdir"]+"/results/tmp/mir_split/{sample}/{sample}_startnt{start}.named.sort.fastq")
    params:
        filename = config["wdir"]+"/results/tmp/mir_split/{sample}/",
        outdir= config["wdir"]+"/results/tmp/mir_split/{sample}/",
        lists = config["wdir"]+"/results/tmp/mir_split/{sample}/{sample}_startnt{start}.list",
        miR_names = config["wdir"]+"/results/tmp/mir_split/{sample}/{sample}_startnt{start}.mirNames.txt",
    threads:1
    resources:
        mem_mb = 40000 # TODO does this work
    conda:
        "envs/racoon_seqkit_v0.2.yml"
    shell:
        """
        if [ -f {params.lists} ]
        then
            seqkit grep -n --quiet -f {params.lists} {input.fastq} > {output.fastq} && \
            echo {output.fastq} | head && \
            seqkit replace -p '(.+)' -r "{{kv}}" -k {params.miR_names} {output.fastq} -o {output.fastq_named} && \
            seqkit sort -n {output.fastq_named} > {output.fastq_sort}
        fi
        """
        # here an empthy output is generated if the file with a certain start position does not exist
        # this will not cause problems because late files of all start positions are merged again

###########################
# cut mirR from the chimeric reads
###########################
## The star position of the miR +21 nt i cut of the chimeric read to obtain the first nt of the protein coding miRNAs
## (mir length = 22nt =  1 (start position) + 21))

rule chimeric_trim:
    input: 
        split_reads = config["wdir"]+"/results/tmp/mir_split/{sample}/{sample}_startnt{start}.named.sort.fastq"
    output:
        trim_reads = touch(config["wdir"]+"/results/tmp/mir_split/{sample}/{sample}_startnt{start}.sort.trim.fastq.gz"),
    params:
        filename= config["wdir"]+"/results/mir_analysis/unaligned_target_RNAs/split_by_mir_start/{sample}/"
    conda:
        "envs/racoon_main_v0.4.yml"
    shell:
        """
        if [ -s {input.split_reads} ]
        then
            echo {wildcards.start} && \
            start=$(( {wildcards.start} + 21 )) && \
            echo $start && \
            fastx_trimmer -f $start -i {input.split_reads} -o {output.trim_reads} -z
        fi
        """

##########################
# trimmed fastq files of chimeric reads are merged again
###########################
rule merge_trimmed:
    input:
        #config["wdir"]+"/results/tmp/mir_split/{sample}"
        expand(config["wdir"]+"/results/tmp/mir_split/{{sample}}/{{sample}}_startnt{start}.sort.trim.fastq.gz", start = MIR_starts)
    output:
        config["wdir"]+"/results/mir_analysis/unaligned_target_RNAs/merged_fastq/{sample}.chim.trim.fastq.gz"
    params:
        starts=MIR_starts,
        path=config["wdir"]+"/results/tmp/mir_split/{sample}/"
    threads: 1
    shell:
        """
        cat {input} > {output}
        """


#########################
# align chimeric (trimmed) reads to genome annotation
#########################

rule align_chimeric:
    input:
        reads=config["wdir"]+"/results/mir_analysis/unaligned_target_RNAs/merged_fastq/{sample}.chim.trim.fastq.gz",
        index_chkpnt= re.sub(r"\.[^.]+$", "", config["gtf"]) + "_idx.chpnt",
        index=re.sub(r"\.[^.]+$", "", config["gtf"]) + "_idx" 
    output:
        bam=config["wdir"]+"/results/mir_analysis/aligned_chimeric_bam/chimeric_{sample}.Aligned.sortedByCoord.out.bam"
    params:
        gtf=config["gtf"],
        wdir=config["wdir"],
        dir="results/mir_analysis/aligned_chimeric_bam/",
        outFilterMismatchNoverReadLmax=config["outFilterMismatchNoverReadLmax"],
        outFilterMismatchNmax=config["outFilterMismatchNmax"],
        outFilterMultimapNmax=config["outFilterMultimapNmax"],
        sjdbOverhang=config["read_length"] - 1 - int(get_barcode_experiment_info()["total_barcode_len"]),
        outReadsUnmapped=config["outReadsUnmapped"],
        outSJfilterReads=config["outSJfilterReads"]
    threads: 2 # x each sample automatically assigned by snakemake
    resources:
        mem_mb = 40000
    conda:
        "envs/racoon_main_v0.4.yml"
    shell:
        """
        chmod +x {input.reads} && \
        mkdir -p {params.wdir}/{params.dir} && \
        chmod -R +x {params.wdir}/{params.dir} && \
        cd {params.wdir}/{params.dir} && \
        STAR --runMode alignReads \
        --genomeDir {input.index} \
        --outFileNamePrefix "chimeric_"{wildcards.sample}. \
        --outFilterMismatchNoverReadLmax {params.outFilterMismatchNoverReadLmax} \
        --outFilterMismatchNmax {params.outFilterMismatchNmax} \
        --outFilterMultimapNmax {params.outFilterMultimapNmax} \
        --alignEndsType "Extend5pOfRead1" \
        --sjdbGTFfile {params.gtf} \
        --sjdbOverhang {params.sjdbOverhang} \
        --outReadsUnmapped {params.outReadsUnmapped} \
        --outSJfilterReads {params.outSJfilterReads} \
        --outSAMtype BAM SortedByCoordinate \
        --readFilesCommand zcat \
        --readFilesIn {input.reads} \
        --runThreadN {threads}
        """

######################
# index bam files of chimeric reads
######################

rule bam_index_chimeric:
    input:
        config["wdir"]+"/results/mir_analysis/aligned_chimeric_bam/chimeric_{sample}.Aligned.sortedByCoord.out.bam"
    output:
        config["wdir"]+"/results/mir_analysis/aligned_chimeric_bam/chimeric_{sample}.Aligned.sortedByCoord.out.bam.bai"
    conda:
        "envs/racoon_samtools.yml"
    threads: 1 # x each sample automatically assigned by snakemake
    shell:
        """
        chmod +x {input} && \
        samtools index {input}
        """

######################
# deduplication of chimeric reads
######################

rule deduplication_chimeric:
    input:
        bam=config["wdir"]+"/results/mir_analysis/aligned_chimeric_bam/chimeric_{sample}.Aligned.sortedByCoord.out.bam",
        bai=config["wdir"]+"/results/mir_analysis/aligned_chimeric_bam/chimeric_{sample}.Aligned.sortedByCoord.out.bam.bai"
    output:
        bam=config["wdir"]+"/results/mir_analysis/aligned_chimeric_bam/chimeric_{sample}.Aligned.sortedByCoord.out.duprm.bam",
        log=config["wdir"]+"/results/mir_analysis/aligned_chimeric_bam/chimeric_{sample}.Aligned.sortedByCoord.out.duprm.log"
    conda:
        "envs/racoon_umi_tools_v0.3.yml"
    threads: 1 # x each sample automatically assigned by snakemake
    shell:
        """
        umi_tools dedup -I {input.bam} -L {output.log} -S {output.bam} --extract-umi-method read_id --method unique
        """


# COMMENTED OUT - sort_bams_chimeric rule is unnecessary for bamtobed
# rule sort_bams_chimeric:
#     input:
#         config["wdir"]+"/results/mir_analysis/aligned_chimeric_bam/chimeric_{sample}.Aligned.sortedByCoord.out.duprm.bam"
#     output:
#         config["wdir"]+"/results/mir_analysis/aligned_chimeric_bam/chimeric_{sample}.Aligned.sortedByCoord.out.duprm.sort.bam"
#     conda:
#         "envs/racoon_main_v0.4.yml"
#     threads: 1 # x each sample automatically assigned by snakemake
#     shell:
#         "samtools sort -n {input} -o {output}"


#######################
# extract crosslinked nucleotides from chimeric reads
#######################
# reads are converted to bed format, shiften 1nt upstream (5' direction) and shortened to 1nt lenght
# the read ID in the bed file is shortened to the mir Name (which was saved in the readID previously )
# bigwig files are made 

rule get_crosslinks_chimeric:
    input:
        genome_chkpnt=config["genome_fasta"]+".fai",
        bam=config["wdir"]+"/results/mir_analysis/aligned_chimeric_bam/chimeric_{sample}.Aligned.sortedByCoord.out.duprm.bam"  # Removed .sort - use coordinate-sorted file
    output:
        bed=config["wdir"]+"/results/tmp/chimeric/chimeric_{sample}.Aligned.sortedByCoord.out.duprm.bed",
        bed_shift=config["wdir"]+"/results/tmp/chimeric/chimeric_{sample}.Aligned.sortedByCoord.out.duprm.shifted.bed",
        bed_shift_named=config["wdir"]+"/results/tmp/chimeric/chimeric_{sample}.Aligned.sortedByCoord.out.duprm.shifted.named.bed",
        bed_shift_for_bw_p=config["wdir"]+"/results/tmp/chimeric/chimeric_{sample}.Aligned.sortedByCoord.out.duprm.shifted.1nt.for_bw.plus.bed",
        bed_shift_for_bw_m=config["wdir"]+"/results/tmp/chimeric/chimeric_{sample}.Aligned.sortedByCoord.out.duprm.shifted.1nt.for_bw.minus.bed",

    params:
        genome_fasta=config["genome_fasta"],
        wdir=config["wdir"],
        dir_groups= config['wdir']+ "/results/groups/",
        crosslink_folder = config['wdir']+"/results/mir_analysis/crosslinks/",
        tmp_folder = config["wdir"]+"/results/tmp/chimeric/",
        file_prefix=config["wdir"]+"/results/mir_analysis/crosslinks/chimeric_{sample}.Aligned.sortedByCoord.out.duprm.shifted.",
        file_prefix_tmp=config["wdir"]+"/results/tmp/chimeric/chimeric_{sample}.Aligned.sortedByCoord.out.duprm.shifted."
    conda:
        "envs/racoon_main_v0.4.yml"
    shell:
        """
        #### Convert all read locations to intervals in bed file format using BEDTools
        bedtools bamtobed -i {input.bam} > {output.bed} \
        \
        #### Shift intervals depending on the strand by 1 bp upstream using BEDTools
        bedtools shift -m 1 -p -1 -i {output.bed} -g {params.genome_fasta}.fai | sort -k1,1 -k2,2n -k3,3n > {output.bed_shift} \
        \
        #### shorten name to miR name
        awk -v FS='\t' -v OFS='\t' '{{sub(/_(.*)/, "", $4) ; print }}' {output.bed_shift} > {output.bed_shift_named} \
        \
        #### split plus and miuns strand
        awk -v FS='\t' -v OFS='\t' '{{ print $1,$2,$3,$4,$5,$6 > "{params.file_prefix_tmp}"$6".bed" }}' {output.bed_shift_named}
        mkdir -p {params.crosslink_folder}
        chmod +x {params.crosslink_folder}
        chmod +x {params.tmp_folder}
        mv {params.file_prefix_tmp}+.bed {params.file_prefix_tmp}plus.bed
        mv {params.file_prefix_tmp}-.bed {params.file_prefix_tmp}minus.bed
        \
        #### keep only 5' end
        awk -v FS='\t' -v OFS='\t' '{{$3 = $2+1; $5 = "1"; print}}' {params.file_prefix_tmp}plus.bed > {params.file_prefix}1nt.plus.bed
        awk -v FS='\t' -v OFS='\t' '{{$3 = $2+1; $5 = "1"; print}}' {params.file_prefix_tmp}minus.bed > {params.file_prefix}1nt.minus.bed
        chmod +x {params.file_prefix}1nt.plus.bed
        chmod +x {params.file_prefix}1nt.minus.bed  
        \
        #### remove names again for bigwig file
        awk -v FS='\t' -v OFS='\t' '{{print $1,$2,$3,$5}}' "{params.file_prefix}"1nt.plus.bed | bedtools sort | bedtools merge -d -1 -c 4 -o sum  > {output.bed_shift_for_bw_p}
        awk -v FS='\t' -v OFS='\t' '{{print $1,$2,$3,$5}}' "{params.file_prefix}"1nt.minus.bed | bedtools sort | bedtools merge -d -1 -c 4 -o sum > {output.bed_shift_for_bw_m}
        """

        # #### convertion of bedgraph files to bw file format files using bedGraphToBigWig of the kentUtils suite
        # bedGraphToBigWig {params.file_prefix_tmp}1nt.for_bw.plus.bed {params.genome_fasta}.fai {output.bw_plus} \
        # bedGraphToBigWig {params.file_prefix_tmp}1nt.for_bw.minus.bed {params.genome_fasta}.fai {output.bw_minus}


rule turn_chimeric_crosslinks_to_bw:
    input:
        bed_shift_for_bw_p=config["wdir"]+"/results/tmp/chimeric/chimeric_{sample}.Aligned.sortedByCoord.out.duprm.shifted.1nt.for_bw.plus.bed.bed",
        bed_shift_for_bw_m=config["wdir"]+"/results/tmp/chimeric/chimeric_{sample}.Aligned.sortedByCoord.out.duprm.shifted.1nt.for_bw.minus.bed.bed",
    output:
        bw_minus=config["wdir"]+"/results/mir_analysis/crosslinks/chimeric_{sample}.Aligned.sortedByCoord.out.duprm.named.1nt.minus.bw",
        bw_plus=config["wdir"]+"/results/mir_analysis/crosslinks/chimeric_{sample}.Aligned.sortedByCoord.out.duprm.named.1nt.plus.bw",
    params:
        genome_fasta=config["genome_fasta"],
    conda:
        "envs/racoon_main_v0.4.yml"
    shell:
        """
        #### conversion of bedgraph files to bw file format files using bedGraphToBigWig of the kentUtils suite
        bedGraphToBigWig {input.bed_shift_for_bw_p} {params.genome_fasta}.fai {output.bw_plus} 
        bedGraphToBigWig {input.bed_shift_for_bw_m} {params.genome_fasta}.fai {output.bw_minus}
        """



####################
# merging of bigwig files from chimeric crosslinks
####################

checkpoint merge_crosslink_bw_chimeric:
    input:
        groups = config["wdir"]+"/results/tmp/groups/{groups}.txt",
        bws = expand(config["wdir"]+"/results/mir_analysis/crosslinks/chimeric_{sample}.Aligned.sortedByCoord.out.duprm.named.1nt.minus.bw", sample = SAMPLES )
    output:
        bed_p=config['wdir']+"/results/mir_analysis/crosslinks_merged/chimeric_{groups}.plus.bedGraph",
        bed_m=config['wdir']+"/results/mir_analysis/crosslinks_merged/chimeric_{groups}.minus.bedGraph",
        bed_p_sort=config['wdir']+"/results/mir_analysis/crosslinks_merged/chimeric_{groups}.sort.plus.bedGraph",
        bed_m_sort=config['wdir']+"/results/mir_analysis/crosslinks_merged/chimeric_{groups}.sort.minus.bedGraph",
        bw_p=config['wdir']+"/results/mir_analysis/crosslinks_merged/chimeric_{groups}.plus.bw",
        bw_m=config['wdir']+"/results/mir_analysis/crosslinks_merged/chimeric_{groups}.minus.bw"
    params:
        genome_fasta=config["genome_fasta"],
        wdir=config["wdir"]
    conda:
        "envs/racoon_ucsc_v0.4.yml"
    shell:
        """
        ### merge plus files per group
        s="$(awk '{{ print "{params.wdir}/results/mir_analysis/crosslinks/chimeric_" $2 ".Aligned.sortedByCoord.out.duprm.named.1nt.plus.bw"}}' {input.groups} )" && \
        echo $s && \
        chmod +x $s && \
        bigWigMerge $s {output.bed_p}  && \
        LC_COLLATE=C sort -k1,1 -k2,2n -k3,3n {output.bed_p} -o {output.bed_p_sort} && \
        bedGraphToBigWig {output.bed_p_sort} {params.genome_fasta}.fai {output.bw_p}
        \
        ### merge minus files per group
        r="$(awk '{{ print "{params.wdir}/results/mir_analysis/crosslinks/chimeric_" $2 ".Aligned.sortedByCoord.out.duprm.named.1nt.minus.bw"}}' {input.groups} )" && \
        echo $r && \
        chmod +x $r && \
        bigWigMerge $r {output.bed_m} && \
        chmod +x {output.bed_m} && \
        LC_COLLATE=C sort -k1,1 -k2,2n -k3,3n {output.bed_m} -o {output.bed_m_sort} && \
        bedGraphToBigWig {output.bed_m_sort} {params.genome_fasta}.fai {output.bw_m}
        """


# -----------------------------
# QCs for miR
# -----------------------------

# .. report miRs

rule make_report_miR:
    input:
        chimeric_stats = config["wdir"]+"/results/mir_analysis/aligned_mir/chimeric_bowtie_stats.txt",
        non_chimeric_stats = config["wdir"]+"/results/mir_analysis/aligned_mir/non_chimeric_bowtie_stats.txt"
    output:
        config["wdir"]+"/results/Report_miR.html"
    params:
        wdir=config["wdir"],
        config= config,
        snake_path= SNAKE_PATH
    conda:
        "envs/racoon_R_v0.4.yml"
    shell:
        """
        $CONDA_PREFIX/lib/R/bin/Rscript {params.snake_path}/workflow/rules/make_report_mir.R {params.wdir} {params.snake_path} "$CONDA_PREFIX" {params.config}
        """



